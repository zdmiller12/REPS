\section{Design Evaluation and Validation}\index{Design Evaluation and Validation}

The basic objectives and benefits of the design review, evaluation, and feedback process are as described in , and the categories (types) and scheduling of formal design reviews are shown in . As mentioned in , the design review and evaluation activity is continuous and includes both (1) an informal ongoing iterative day-to-day process of review and evaluation, and (2) the scheduling of formal design reviews at discrete points in design and throughout system acquisition. The purpose of conducting any type of a review is to assess if (and just how well) the design configuration, as envisioned at the time, is in compliance with the initially specified quantitative and qualitative requirements. The technical performance measures (TPMs), identified and prioritized in the conceptual design phase (refer to ) and allocated to the various elements of the system (refer to ), must be measured to ensure compliance with these specified requirements. Further, these critical measures must be ``tracked'' from one review to the next.

Five key TPMs have been selected in to illustrate the tracking of TPMs. It is assumed that the specific quantitative requirements have been established, along with an allowable “target” range for each of the values. For instance, the system operational availability (Ao) must be at least 0.90, but can be higher if feasible. As a result of a system design review, based on a prediction associated with the design configuration at the time, it was determined that Ao was only around 0.82. Given this, there is a need to develop a formal design plan indicating the steps that will be necessary to realize the required reliability growth in order to comply with the 0.90 requirement. Each of the specified TPMs must be monitored in a similar manner.

For relatively large systems, there may be several distinct TPM requirements that must be met and there should be priorities established to indicate relative degrees of importance. While all requirements may be important, there is a tendency on the part of some designers to favor one TPM requirement over another. Given that system design requires a ``team'' approach and that the different team members may represent different design disciplines, there may be a greater amount of effort expended to ensure that the design will meet one requirement at the expense of not complying with another. For instance, if the system being developed is an aerospace system, then weight, size, and speed are critical and the impact from an aeronautical engineering perspective is high. On the other hand, meeting an MLH/OH requirement may not be of concern to the aeronautical engineer assigned to the design team, but is highly important when considering the maintainability characteristics in design.

To ensure that all of the requirements are met, or at least seriously addressed, various design team members may be assigned to ``track'' specific TPMs throughout the design process. illustrates the relationships between TPMs and responsible design disciplines (in terms of likely interest) and indicates where the ``tracking'' assignments may be specified. In other words, a reliability engineer will likely ``track'' the requirements pertaining to the MTBF, a maintainability engineer will be interested in MLH/OH, MTBM, and LCC, and so on. From a systems engineering perspective, all of these factors must be ``tracked'' to the extent indicated by the distributed priorities.

As the designer evolves through the detail design and development process, including the conduct of the essential trade-off analyses leading to the selection of specific components, he/she must be cognizant of considerations other than the obvious TPMs. Two of these are presented here as examples.

The expected life of each of the components chosen for incorporation into the design is a consideration of importance. If the specified component ``life'' is less than the planned life cycle of the system (i.e., a ``critical useful-life item''), then this leads to the requirement for a scheduled maintenance plan for component replacement. If, when combining and integrating the various components into a larger assembly, the expected life is less than the planned life cycle of the system, then once again there may be a need for a preplanned system maintenance cycle or a redesign for mitigation of this burden. In any case, the designer must track and address the issue of obsolescence in design (as in ) ``before the fact'' instead of leaving it to chance later downstream in the system life cycle.

Another consideration to be tracked in today’s environment pertains to implementing the overall system design process effectively, in a limited amount of time, and at reduced cost. Shortening the acquisition process (cycle) continues to be a desired objective. At the same time, there has been a tendency to continue the design process, on an evolutionary basis, for as long as possible and incorporate design improvements up to the last minute before delivering the system to the customer. As desirable as this might appear, a superb system delivered late and over budget may be considered a failure by the customer.

Design reviews are scheduled periodically. Any review depends on the depth of planning, organization, and preparation prior to the review itself. An extensive amount of coordination is needed, involving the following factors:

\begin{enumerate}
\item Identification of the items to be reviewed
\item A selected date for the review
\item The location or facility where the review is to be conducted
\item An agenda for the review (including a definition of the basic objectives)
\item A design review board representing the organizational elements and disciplines affected by the review. Basic design functions, reliability, maintainability, human factors, quality control, manufacturing, sustainability, and logistic support representation are included. Individual organizational responsibilities should be identified. Depending on the type of review, the customer and/or individual equipment suppliers may be included
\item Equipment (hardware) and/or software requirements for the review. Engineering models, prototypes and/or mock-ups may be required to facilitate the review process
\item Design data requirements for the review. This may include all applicable specifications, lists, drawings, predictions and analyses, logistic data, computer data, and special reports
\item Funding requirements. Planning is necessary in identifying sources and a means for providing the funds for conducting the review
\item Reporting requirements and the mechanism for accomplishing the necessary follow-up actions stemming from design review recommendations. Responsibilities and action-item time limits must be established
\end{enumerate}

The design review involves a number of different discipline areas and covers a wide variety of design data and in some instances the presence of hardware, software, and/or other selected elements of the system. In order to fulfill its objective expeditiously (i.e., review the design to ensure that all system requirements are met in an optimum manner), the design review must be well organized and firmly controlled by the design review board chairperson. Design review meetings should be brief and to the point and must not be allowed to drift away from the topics on the agenda. Attendance should be limited to those who have a direct interest and can contribute to the subject matter being presented. Specialists who participate should be authorized to speak and make decisions concerning their area of specialty. Finally, the design review must make provisions for the identification, recording, scheduling, and monitoring of any subsequent corrective action that is required. Specific responsibility for follow-up action must be designated by the chairperson of the design review board.

I know about exercising a) the system model then b) the system as is being realized then c) the system as interoperating with its real context all for the purpose of discovering dynamic and integrity limits. The key is the viability of the test scenarios. In a system consisting of hundreds of variables and hundreds factorial of data states no contrived test cases are going to find the integrity limits even if they cover 100\% of the algorithms.
Is it time to understand systems integrity assessment as an analysis practice. J

System test, evaluation, and validation activities should be established during the conceptual design phase of the life cycle, concurrently with the definition of the overall system design requirements. From that point on, the test and evaluation effort continues by the testing of individual components, the testing of various system elements and major subsystems, and then by the testing of the overall system as an integrated entity. The objective is to adopt a ``progressive'' approach that will lend itself to continuous implementation and improvement as the system design and development process evolves.

Test and evaluation activities discussed in this chapter can be aligned initially with the design activities described in and then extended through the production/construction and the system utilization and support phases. These chapters address an evolutionary treatment of the system design process and ``look ahead'' to downstream outcomes where the results will have defined a specific configuration, supported by a comprehensive design database and augmented by supplemental analyses.

The next step is that of validation. Validation, as defined herein, refers to the steps and the process needed to ensure that the system configuration, as designed, meets all requirements initially specified by the customer. The process of validation is somewhat evolutionary in nature. Referring to , the activities noted by Blocks 0.6, 1.5, 2.3, and 8.1.4 invoke an ongoing process of review, evaluation, feedback, and the ultimate verification of requirements. These activities have been primarily directed to address early design concepts and various system elements. However, up to this point, a total integrated approach for the validation of the system and its elements, as an integrated entity, has not been fully accomplished. Final system validation occurs when the system performs effectively and efficiently when operating within its associated higher-level system-of-systems (SOS) configuration (as applicable).

The purpose of this chapter is to present an integrated approach for system test and evaluation and to facilitate the necessary validation of the proposed system configuration to provide assurance that it will indeed meet customer requirements. This chapter addresses the following topics:

\begin{itemize}
\item Determining the requirements for system test, evaluation, and validation
\item Describing the categories of system test and evaluation
\item Planning for system test and evaluation
\item Preparing for system test and evaluation
\item Conducting the system test, collecting and analyzing the test data, comparing the results with the initially specified requirements, preparing a test report
\item Incorporating system modifications as required
\end{itemize}

Upon completion of this chapter, the reader should have acquired an understanding of how the evaluation and validation of system design should be accomplished. A section on summary and extensions closes the chapter, with suggested references for further study also provided. Also, several relevant website addresses are offered.